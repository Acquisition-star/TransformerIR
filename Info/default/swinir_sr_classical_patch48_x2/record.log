[2025-01-23 20:01:10 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-23 20:01:10 swinir_sr_classical_patch48_x2] (main.py 60): INFO datasets:
  test:
    dataroot_H: E:\Data\Set14\original
    dataroot_L: E:\Data\Set14\LRbicx2
    dataset_type: sr
    name: test_dataset
  train:
    H_size: 96
    dataloader_batch_size: 32
    dataloader_num_workers: 16
    dataroot_H: E:\Data\DIV2K\DIV2K_train_HR
    dataroot_L: E:\Data\DIV2K\DIV2K_train_LR_unknown\X2
    dataset_type: sr
    name: train_dataset
dist: false
gpu_ids:
- 0
- 1
model: swinir
n_channels: 3
netG:
  depths:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  embed_dim: 180
  img_range: 1.0
  img_size: 48
  in_chans: 3
  init_type: default
  mlp_ratio: 2
  name: swinir
  resi_connection: 1conv
  upsampler: pixelshuffle
  upscale: 2
  window_size: 8
path:
  checkpoint_path: Info\default\swinir_sr_classical_patch48_x2\checkpoints
  config_path: Info\default\swinir_sr_classical_patch48_x2\config.json
  pretrained_net: null
  root_path: Info\default\swinir_sr_classical_patch48_x2
scale: 2
seed: 0
task: swinir_sr_classical_patch48_x2
train:
  E_decay: 0.999
  E_param_strict: true
  G_lossfn_type: l1
  G_lossfn_weight: 1.0
  G_optimizer_clipgrad: null
  G_optimizer_lr: 0.0002
  G_optimizer_reuse: true
  G_optimizer_type: adam
  G_optimizer_wd: 0
  G_param_strict: true
  G_regularizer_clipstep: null
  G_regularizer_orthstep: null
  G_scheduler_gamma: 0.5
  G_scheduler_milestones:
  - 250000
  - 400000
  - 450000
  - 475000
  - 500000
  G_scheduler_type: MultiStepLR
  checkpoint_print: 200
  checkpoint_save: 5000
  checkpoint_test: 5000

[2025-01-23 20:01:10 swinir_sr_classical_patch48_x2] (main.py 61): INFO {"cfg": "configs/SR/X2/train_swinir_sr_classical_x2.yaml", "output": "Info/", "env": "default", "dist": false}
[2025-01-23 20:01:10 swinir_sr_classical_patch48_x2] (main_train.py 7): INFO hello
[2025-01-25 00:08:02 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 00:08:02 swinir_sr_classical_patch48_x2] (main.py 60): INFO datasets:
  dataloader_batch_size: 32
  dataloader_num_workers: 8
  dataloader_shuffle: true
  dataset_type: sr
  train:
    H_size: 96
    dataroot_H: E:\Data\DIV2K\DIV2K_train_HR
    dataroot_L: E:\Data\DIV2K\DIV2K_train_LR_unknown\X2
  val:
    dataroot_H: E:\Data\Set14\original
    dataroot_L: E:\Data\Set14\LRbicx2
dist: false
gpu_ids:
- 0
- 1
model: swinir
n_channels: 3
netG:
  depths:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  embed_dim: 180
  img_range: 1.0
  img_size: 48
  in_chans: 3
  init_type: default
  mlp_ratio: 2
  name: swinir
  resi_connection: 1conv
  upsampler: pixelshuffle
  upscale: 2
  window_size: 8
path:
  checkpoint_path: Info\default\swinir_sr_classical_patch48_x2\checkpoints
  config_path: Info\default\swinir_sr_classical_patch48_x2\config.json
  pretrained_net: null
  root_path: Info\default\swinir_sr_classical_patch48_x2
scale: 2
seed: 0
task: swinir_sr_classical_patch48_x2
train:
  E_decay: 0.999
  E_param_strict: true
  G_lossfn_type: l1
  G_lossfn_weight: 1.0
  G_optimizer_clipgrad: null
  G_optimizer_lr: 0.0002
  G_optimizer_reuse: true
  G_optimizer_type: adam
  G_optimizer_wd: 0
  G_param_strict: true
  G_regularizer_clipstep: null
  G_regularizer_orthstep: null
  G_scheduler_gamma: 0.5
  G_scheduler_milestones:
  - 250000
  - 400000
  - 450000
  - 475000
  - 500000
  G_scheduler_type: MultiStepLR
  checkpoint_print: 200
  checkpoint_save: 5000
  checkpoint_test: 5000

[2025-01-25 00:08:02 swinir_sr_classical_patch48_x2] (main.py 61): INFO {"cfg": "configs/SR/X2/train_swinir_sr_classical_x2.yaml", "output": "Info/", "env": "default", "dist": false}
[2025-01-25 00:08:36 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 00:08:36 swinir_sr_classical_patch48_x2] (main.py 60): INFO datasets:
  dataloader_batch_size: 32
  dataloader_num_workers: 8
  dataloader_shuffle: true
  dataset_type: sr
  train:
    H_size: 96
    dataroot_H: E:\Data\DIV2K\DIV2K_train_HR
    dataroot_L: E:\Data\DIV2K\DIV2K_train_LR_unknown\X2
  val:
    dataroot_H: E:\Data\Set14\original
    dataroot_L: E:\Data\Set14\LRbicx2
dist: false
gpu_ids:
- 0
- 1
model: swinir
n_channels: 3
netG:
  depths:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  embed_dim: 180
  img_range: 1.0
  img_size: 48
  in_chans: 3
  init_type: default
  mlp_ratio: 2
  name: swinir
  resi_connection: 1conv
  upsampler: pixelshuffle
  upscale: 2
  window_size: 8
path:
  checkpoint_path: Info\default\swinir_sr_classical_patch48_x2\checkpoints
  config_path: Info\default\swinir_sr_classical_patch48_x2\config.json
  pretrained_net: null
  root_path: Info\default\swinir_sr_classical_patch48_x2
scale: 2
seed: 0
task: swinir_sr_classical_patch48_x2
train:
  E_decay: 0.999
  E_param_strict: true
  G_lossfn_type: l1
  G_lossfn_weight: 1.0
  G_optimizer_clipgrad: null
  G_optimizer_lr: 0.0002
  G_optimizer_reuse: true
  G_optimizer_type: adam
  G_optimizer_wd: 0
  G_param_strict: true
  G_regularizer_clipstep: null
  G_regularizer_orthstep: null
  G_scheduler_gamma: 0.5
  G_scheduler_milestones:
  - 250000
  - 400000
  - 450000
  - 475000
  - 500000
  G_scheduler_type: MultiStepLR
  checkpoint_print: 200
  checkpoint_save: 5000
  checkpoint_test: 5000

[2025-01-25 00:08:36 swinir_sr_classical_patch48_x2] (main.py 61): INFO {"cfg": "configs/SR/X2/train_swinir_sr_classical_x2.yaml", "output": "Info/", "env": "default", "dist": false}
[2025-01-25 00:14:28 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 00:14:28 swinir_sr_classical_patch48_x2] (main.py 60): INFO datasets:
  dataloader_batch_size: 32
  dataloader_num_workers: 8
  dataloader_shuffle: true
  dataset_type: sr
  train:
    H_size: 96
    dataroot_H: E:\Data\DIV2K\DIV2K_train_HR
    dataroot_L: E:\Data\DIV2K\DIV2K_train_LR_unknown\X2
  val:
    dataroot_H: E:\Data\Set14\original
    dataroot_L: E:\Data\Set14\LRbicx2
dist: false
gpu_ids:
- 0
- 1
model: swinir
n_channels: 3
netG:
  depths:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  embed_dim: 180
  img_range: 1.0
  img_size: 48
  in_chans: 3
  init_type: default
  mlp_ratio: 2
  name: swinir
  resi_connection: 1conv
  upsampler: pixelshuffle
  upscale: 2
  window_size: 8
path:
  checkpoint_path: Info\default\swinir_sr_classical_patch48_x2\checkpoints
  config_path: Info\default\swinir_sr_classical_patch48_x2\config.json
  pretrained_net: null
  root_path: Info\default\swinir_sr_classical_patch48_x2
scale: 2
seed: 0
task: swinir_sr_classical_patch48_x2
train:
  E_decay: 0.999
  E_param_strict: true
  G_lossfn_type: l1
  G_lossfn_weight: 1.0
  G_optimizer_clipgrad: null
  G_optimizer_lr: 0.0002
  G_optimizer_reuse: true
  G_optimizer_type: adam
  G_optimizer_wd: 0
  G_param_strict: true
  G_regularizer_clipstep: null
  G_regularizer_orthstep: null
  G_scheduler_gamma: 0.5
  G_scheduler_milestones:
  - 250000
  - 400000
  - 450000
  - 475000
  - 500000
  G_scheduler_type: MultiStepLR
  checkpoint_print: 200
  checkpoint_save: 5000
  checkpoint_test: 5000

[2025-01-25 00:14:28 swinir_sr_classical_patch48_x2] (main.py 61): INFO {"cfg": "configs/SR/X2/train_swinir_sr_classical_x2.yaml", "output": "Info/", "env": "default", "dist": false}
[2025-01-25 00:14:58 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 00:14:58 swinir_sr_classical_patch48_x2] (main.py 60): INFO datasets:
  dataloader_batch_size: 32
  dataloader_num_workers: 8
  dataloader_shuffle: true
  dataset_type: sr
  train:
    H_size: 96
    dataroot_H: E:\Data\DIV2K\DIV2K_train_HR
    dataroot_L: E:\Data\DIV2K\DIV2K_train_LR_unknown\X2
  val:
    dataroot_H: E:\Data\Set14\original
    dataroot_L: E:\Data\Set14\LRbicx2
dist: false
gpu_ids:
- 0
- 1
model: swinir
n_channels: 3
netG:
  depths:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  embed_dim: 180
  img_range: 1.0
  img_size: 48
  in_chans: 3
  init_type: default
  mlp_ratio: 2
  name: swinir
  resi_connection: 1conv
  upsampler: pixelshuffle
  upscale: 2
  window_size: 8
path:
  checkpoint_path: Info\default\swinir_sr_classical_patch48_x2\checkpoints
  config_path: Info\default\swinir_sr_classical_patch48_x2\config.json
  pretrained_net: null
  root_path: Info\default\swinir_sr_classical_patch48_x2
scale: 2
seed: 0
task: swinir_sr_classical_patch48_x2
train:
  E_decay: 0.999
  E_param_strict: true
  G_lossfn_type: l1
  G_lossfn_weight: 1.0
  G_optimizer_clipgrad: null
  G_optimizer_lr: 0.0002
  G_optimizer_reuse: true
  G_optimizer_type: adam
  G_optimizer_wd: 0
  G_param_strict: true
  G_regularizer_clipstep: null
  G_regularizer_orthstep: null
  G_scheduler_gamma: 0.5
  G_scheduler_milestones:
  - 250000
  - 400000
  - 450000
  - 475000
  - 500000
  G_scheduler_type: MultiStepLR
  checkpoint_print: 200
  checkpoint_save: 5000
  checkpoint_test: 5000

[2025-01-25 00:14:58 swinir_sr_classical_patch48_x2] (main.py 61): INFO {"cfg": "configs/SR/X2/train_swinir_sr_classical_x2.yaml", "output": "Info/", "env": "default", "dist": false}
[2025-01-25 00:15:45 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 00:15:45 swinir_sr_classical_patch48_x2] (main.py 60): INFO datasets:
  dataloader_batch_size: 32
  dataloader_num_workers: 8
  dataloader_shuffle: true
  dataset_type: sr
  train:
    H_size: 96
    dataroot_H: E:\Data\DIV2K\DIV2K_train_HR
    dataroot_L: E:\Data\DIV2K\DIV2K_train_LR_unknown\X2
  val:
    dataroot_H: E:\Data\Set14\original
    dataroot_L: E:\Data\Set14\LRbicx2
dist: false
gpu_ids:
- 0
- 1
model: swinir
n_channels: 3
netG:
  depths:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  embed_dim: 180
  img_range: 1.0
  img_size: 48
  in_chans: 3
  init_type: default
  mlp_ratio: 2
  name: swinir
  resi_connection: 1conv
  upsampler: pixelshuffle
  upscale: 2
  window_size: 8
path:
  checkpoint_path: Info\default\swinir_sr_classical_patch48_x2\checkpoints
  config_path: Info\default\swinir_sr_classical_patch48_x2\config.json
  pretrained_net: null
  root_path: Info\default\swinir_sr_classical_patch48_x2
scale: 2
seed: 0
task: swinir_sr_classical_patch48_x2
train:
  E_decay: 0.999
  E_param_strict: true
  G_lossfn_type: l1
  G_lossfn_weight: 1.0
  G_optimizer_clipgrad: null
  G_optimizer_lr: 0.0002
  G_optimizer_reuse: true
  G_optimizer_type: adam
  G_optimizer_wd: 0
  G_param_strict: true
  G_regularizer_clipstep: null
  G_regularizer_orthstep: null
  G_scheduler_gamma: 0.5
  G_scheduler_milestones:
  - 250000
  - 400000
  - 450000
  - 475000
  - 500000
  G_scheduler_type: MultiStepLR
  checkpoint_print: 200
  checkpoint_save: 5000
  checkpoint_test: 5000

[2025-01-25 00:15:45 swinir_sr_classical_patch48_x2] (main.py 61): INFO {"cfg": "configs/SR/X2/train_swinir_sr_classical_x2.yaml", "output": "Info/", "env": "default", "dist": false}
[2025-01-25 00:16:00 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 00:16:00 swinir_sr_classical_patch48_x2] (main.py 60): INFO datasets:
  dataloader_batch_size: 32
  dataloader_num_workers: 8
  dataloader_shuffle: true
  dataset_type: sr
  train:
    H_size: 96
    dataroot_H: E:\Data\DIV2K\DIV2K_train_HR
    dataroot_L: E:\Data\DIV2K\DIV2K_train_LR_unknown\X2
  val:
    dataroot_H: E:\Data\Set14\original
    dataroot_L: E:\Data\Set14\LRbicx2
dist: false
gpu_ids:
- 0
- 1
model: swinir
n_channels: 3
netG:
  depths:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  embed_dim: 180
  img_range: 1.0
  img_size: 48
  in_chans: 3
  init_type: default
  mlp_ratio: 2
  name: swinir
  resi_connection: 1conv
  upsampler: pixelshuffle
  upscale: 2
  window_size: 8
path:
  checkpoint_path: Info\default\swinir_sr_classical_patch48_x2\checkpoints
  config_path: Info\default\swinir_sr_classical_patch48_x2\config.json
  pretrained_net: null
  root_path: Info\default\swinir_sr_classical_patch48_x2
scale: 2
seed: 0
task: swinir_sr_classical_patch48_x2
train:
  E_decay: 0.999
  E_param_strict: true
  G_lossfn_type: l1
  G_lossfn_weight: 1.0
  G_optimizer_clipgrad: null
  G_optimizer_lr: 0.0002
  G_optimizer_reuse: true
  G_optimizer_type: adam
  G_optimizer_wd: 0
  G_param_strict: true
  G_regularizer_clipstep: null
  G_regularizer_orthstep: null
  G_scheduler_gamma: 0.5
  G_scheduler_milestones:
  - 250000
  - 400000
  - 450000
  - 475000
  - 500000
  G_scheduler_type: MultiStepLR
  checkpoint_print: 200
  checkpoint_save: 5000
  checkpoint_test: 5000

[2025-01-25 00:16:00 swinir_sr_classical_patch48_x2] (main.py 61): INFO {"cfg": "configs/SR/X2/train_swinir_sr_classical_x2.yaml", "output": "Info/", "env": "default", "dist": false}
[2025-01-25 00:16:42 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 00:16:42 swinir_sr_classical_patch48_x2] (main.py 60): INFO datasets:
  dataloader_batch_size: 32
  dataloader_num_workers: 8
  dataloader_shuffle: true
  dataset_type: sr
  train:
    H_size: 96
    dataroot_H: E:\Data\DIV2K\DIV2K_train_HR
    dataroot_L: E:\Data\DIV2K\DIV2K_train_LR_unknown\X2
  val:
    dataroot_H: E:\Data\Set14\original
    dataroot_L: E:\Data\Set14\LRbicx2
dist: false
gpu_ids:
- 0
- 1
model: swinir
n_channels: 3
netG:
  depths:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  embed_dim: 180
  img_range: 1.0
  img_size: 48
  in_chans: 3
  init_type: default
  mlp_ratio: 2
  name: swinir
  resi_connection: 1conv
  upsampler: pixelshuffle
  upscale: 2
  window_size: 8
path:
  checkpoint_path: Info\default\swinir_sr_classical_patch48_x2\checkpoints
  config_path: Info\default\swinir_sr_classical_patch48_x2\config.json
  pretrained_net: null
  root_path: Info\default\swinir_sr_classical_patch48_x2
scale: 2
seed: 0
task: swinir_sr_classical_patch48_x2
train:
  E_decay: 0.999
  E_param_strict: true
  G_lossfn_type: l1
  G_lossfn_weight: 1.0
  G_optimizer_clipgrad: null
  G_optimizer_lr: 0.0002
  G_optimizer_reuse: true
  G_optimizer_type: adam
  G_optimizer_wd: 0
  G_param_strict: true
  G_regularizer_clipstep: null
  G_regularizer_orthstep: null
  G_scheduler_gamma: 0.5
  G_scheduler_milestones:
  - 250000
  - 400000
  - 450000
  - 475000
  - 500000
  G_scheduler_type: MultiStepLR
  checkpoint_print: 200
  checkpoint_save: 5000
  checkpoint_test: 5000

[2025-01-25 00:16:42 swinir_sr_classical_patch48_x2] (main.py 61): INFO {"cfg": "configs/SR/X2/train_swinir_sr_classical_x2.yaml", "output": "Info/", "env": "default", "dist": false}
[2025-01-25 00:51:31 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 00:51:31 swinir_sr_classical_patch48_x2] (main.py 60): INFO datasets:
  dataloader_batch_size: 32
  dataloader_num_workers: 8
  dataloader_shuffle: true
  dataset_type: sr
  train:
    H_size: 96
    dataroot_H: E:\Data\DIV2K\DIV2K_train_HR
    dataroot_L: E:\Data\DIV2K\DIV2K_train_LR_unknown\X2
  val:
    dataroot_H: E:\Data\Set14\original
    dataroot_L: E:\Data\Set14\LRbicx2
dist: false
gpu_ids:
- 0
- 1
model: swinir
n_channels: 3
net:
  depths:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  embed_dim: 180
  img_range: 1.0
  img_size: 48
  in_chans: 3
  init_type: default
  mlp_ratio: 2
  num_heads:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  resi_connection: 1conv
  type: swinir
  upsampler: pixelshuffle
  upscale: 2
  window_size: 8
path:
  checkpoint_path: Info\default\swinir_sr_classical_patch48_x2\checkpoints
  config_path: Info\default\swinir_sr_classical_patch48_x2\config.json
  pretrained_net: null
  root_path: Info\default\swinir_sr_classical_patch48_x2
scale: 2
seed: 0
task: swinir_sr_classical_patch48_x2
train:
  E_decay: 0.999
  E_param_strict: true
  G_lossfn_type: l1
  G_lossfn_weight: 1.0
  G_optimizer_clipgrad: null
  G_optimizer_lr: 0.0002
  G_optimizer_reuse: true
  G_optimizer_type: adam
  G_optimizer_wd: 0
  G_param_strict: true
  G_regularizer_clipstep: null
  G_regularizer_orthstep: null
  G_scheduler_gamma: 0.5
  G_scheduler_milestones:
  - 250000
  - 400000
  - 450000
  - 475000
  - 500000
  G_scheduler_type: MultiStepLR
  checkpoint_print: 200
  checkpoint_save: 5000
  checkpoint_test: 5000

[2025-01-25 00:51:31 swinir_sr_classical_patch48_x2] (main.py 61): INFO {"cfg": "configs/SR/X2/train_swinir_sr_classical_x2.yaml", "output": "Info/", "env": "default", "dist": false}
[2025-01-25 00:51:39 swinir_sr_classical_patch48_x2] (main_train.py 14): INFO Building model: {config.net.type} / {config.task}
[2025-01-25 00:52:27 swinir_sr_classical_patch48_x2] (main_train.py 16): INFO SwinIR(
  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.003)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.006)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.009)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.011)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.014)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.017)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.020)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.023)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.026)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.029)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.031)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.034)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.037)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.040)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.043)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.046)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.049)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.051)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.054)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.057)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.060)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.063)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.066)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.069)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.071)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.074)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.077)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.080)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.083)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.086)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.089)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.091)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.094)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.097)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.100)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
[2025-01-25 00:53:07 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 00:53:07 swinir_sr_classical_patch48_x2] (main.py 60): INFO datasets:
  dataloader_batch_size: 32
  dataloader_num_workers: 8
  dataloader_shuffle: true
  dataset_type: sr
  train:
    H_size: 96
    dataroot_H: E:\Data\DIV2K\DIV2K_train_HR
    dataroot_L: E:\Data\DIV2K\DIV2K_train_LR_unknown\X2
  val:
    dataroot_H: E:\Data\Set14\original
    dataroot_L: E:\Data\Set14\LRbicx2
dist: false
gpu_ids:
- 0
- 1
model: swinir
n_channels: 3
net:
  depths:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  embed_dim: 180
  img_range: 1.0
  img_size: 48
  in_chans: 3
  init_type: default
  mlp_ratio: 2
  num_heads:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  resi_connection: 1conv
  type: swinir
  upsampler: pixelshuffle
  upscale: 2
  window_size: 8
path:
  checkpoint_path: Info\default\swinir_sr_classical_patch48_x2\checkpoints
  config_path: Info\default\swinir_sr_classical_patch48_x2\config.json
  pretrained_net: null
  root_path: Info\default\swinir_sr_classical_patch48_x2
scale: 2
seed: 0
task: swinir_sr_classical_patch48_x2
train:
  E_decay: 0.999
  E_param_strict: true
  G_lossfn_type: l1
  G_lossfn_weight: 1.0
  G_optimizer_clipgrad: null
  G_optimizer_lr: 0.0002
  G_optimizer_reuse: true
  G_optimizer_type: adam
  G_optimizer_wd: 0
  G_param_strict: true
  G_regularizer_clipstep: null
  G_regularizer_orthstep: null
  G_scheduler_gamma: 0.5
  G_scheduler_milestones:
  - 250000
  - 400000
  - 450000
  - 475000
  - 500000
  G_scheduler_type: MultiStepLR
  checkpoint_print: 200
  checkpoint_save: 5000
  checkpoint_test: 5000

[2025-01-25 00:53:07 swinir_sr_classical_patch48_x2] (main.py 61): INFO {"cfg": "configs/SR/X2/train_swinir_sr_classical_x2.yaml", "output": "Info/", "env": "default", "dist": false}
[2025-01-25 00:53:07 swinir_sr_classical_patch48_x2] (main_train.py 14): INFO Building model: {config.net.type} / {config.task}
[2025-01-25 00:53:55 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 00:53:55 swinir_sr_classical_patch48_x2] (main.py 60): INFO datasets:
  dataloader_batch_size: 32
  dataloader_num_workers: 8
  dataloader_shuffle: true
  dataset_type: sr
  train:
    H_size: 96
    dataroot_H: E:\Data\DIV2K\DIV2K_train_HR
    dataroot_L: E:\Data\DIV2K\DIV2K_train_LR_unknown\X2
  val:
    dataroot_H: E:\Data\Set14\original
    dataroot_L: E:\Data\Set14\LRbicx2
dist: false
gpu_ids:
- 0
- 1
model: swinir
n_channels: 3
net:
  depths:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  embed_dim: 180
  img_range: 1.0
  img_size: 48
  in_chans: 3
  init_type: default
  mlp_ratio: 2
  num_heads:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  resi_connection: 1conv
  type: swinir
  upsampler: pixelshuffle
  upscale: 2
  window_size: 8
path:
  checkpoint_path: Info\default\swinir_sr_classical_patch48_x2\checkpoints
  config_path: Info\default\swinir_sr_classical_patch48_x2\config.json
  pretrained_net: null
  root_path: Info\default\swinir_sr_classical_patch48_x2
scale: 2
seed: 0
task: swinir_sr_classical_patch48_x2
train:
  E_decay: 0.999
  E_param_strict: true
  G_lossfn_type: l1
  G_lossfn_weight: 1.0
  G_optimizer_clipgrad: null
  G_optimizer_lr: 0.0002
  G_optimizer_reuse: true
  G_optimizer_type: adam
  G_optimizer_wd: 0
  G_param_strict: true
  G_regularizer_clipstep: null
  G_regularizer_orthstep: null
  G_scheduler_gamma: 0.5
  G_scheduler_milestones:
  - 250000
  - 400000
  - 450000
  - 475000
  - 500000
  G_scheduler_type: MultiStepLR
  checkpoint_print: 200
  checkpoint_save: 5000
  checkpoint_test: 5000

[2025-01-25 00:53:55 swinir_sr_classical_patch48_x2] (main.py 61): INFO {"cfg": "configs/SR/X2/train_swinir_sr_classical_x2.yaml", "output": "Info/", "env": "default", "dist": false}
[2025-01-25 00:53:55 swinir_sr_classical_patch48_x2] (main_train.py 14): INFO Building model: {config.net.type} / {config.task}
[2025-01-25 00:54:15 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 00:54:15 swinir_sr_classical_patch48_x2] (main.py 60): INFO datasets:
  dataloader_batch_size: 32
  dataloader_num_workers: 8
  dataloader_shuffle: true
  dataset_type: sr
  train:
    H_size: 96
    dataroot_H: E:\Data\DIV2K\DIV2K_train_HR
    dataroot_L: E:\Data\DIV2K\DIV2K_train_LR_unknown\X2
  val:
    dataroot_H: E:\Data\Set14\original
    dataroot_L: E:\Data\Set14\LRbicx2
dist: false
gpu_ids:
- 0
- 1
model: swinir
n_channels: 3
net:
  depths:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  embed_dim: 180
  img_range: 1.0
  img_size: 48
  in_chans: 3
  init_type: default
  mlp_ratio: 2
  num_heads:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  resi_connection: 1conv
  type: swinir
  upsampler: pixelshuffle
  upscale: 2
  window_size: 8
path:
  checkpoint_path: Info\default\swinir_sr_classical_patch48_x2\checkpoints
  config_path: Info\default\swinir_sr_classical_patch48_x2\config.json
  pretrained_net: null
  root_path: Info\default\swinir_sr_classical_patch48_x2
scale: 2
seed: 0
task: swinir_sr_classical_patch48_x2
train:
  E_decay: 0.999
  E_param_strict: true
  G_lossfn_type: l1
  G_lossfn_weight: 1.0
  G_optimizer_clipgrad: null
  G_optimizer_lr: 0.0002
  G_optimizer_reuse: true
  G_optimizer_type: adam
  G_optimizer_wd: 0
  G_param_strict: true
  G_regularizer_clipstep: null
  G_regularizer_orthstep: null
  G_scheduler_gamma: 0.5
  G_scheduler_milestones:
  - 250000
  - 400000
  - 450000
  - 475000
  - 500000
  G_scheduler_type: MultiStepLR
  checkpoint_print: 200
  checkpoint_save: 5000
  checkpoint_test: 5000

[2025-01-25 00:54:15 swinir_sr_classical_patch48_x2] (main.py 61): INFO {"cfg": "configs/SR/X2/train_swinir_sr_classical_x2.yaml", "output": "Info/", "env": "default", "dist": false}
[2025-01-25 00:54:15 swinir_sr_classical_patch48_x2] (main_train.py 14): INFO Building model: swinir / swinir_sr_classical_patch48_x2
[2025-01-25 00:54:33 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 00:54:33 swinir_sr_classical_patch48_x2] (main.py 60): INFO datasets:
  dataloader_batch_size: 32
  dataloader_num_workers: 8
  dataloader_shuffle: true
  dataset_type: sr
  train:
    H_size: 96
    dataroot_H: E:\Data\DIV2K\DIV2K_train_HR
    dataroot_L: E:\Data\DIV2K\DIV2K_train_LR_unknown\X2
  val:
    dataroot_H: E:\Data\Set14\original
    dataroot_L: E:\Data\Set14\LRbicx2
dist: false
gpu_ids:
- 0
- 1
model: swinir
n_channels: 3
net:
  depths:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  embed_dim: 180
  img_range: 1.0
  img_size: 48
  in_chans: 3
  init_type: default
  mlp_ratio: 2
  num_heads:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  resi_connection: 1conv
  type: swinir
  upsampler: pixelshuffle
  upscale: 2
  window_size: 8
path:
  checkpoint_path: Info\default\swinir_sr_classical_patch48_x2\checkpoints
  config_path: Info\default\swinir_sr_classical_patch48_x2\config.json
  pretrained_net: null
  root_path: Info\default\swinir_sr_classical_patch48_x2
scale: 2
seed: 0
task: swinir_sr_classical_patch48_x2
train:
  E_decay: 0.999
  E_param_strict: true
  G_lossfn_type: l1
  G_lossfn_weight: 1.0
  G_optimizer_clipgrad: null
  G_optimizer_lr: 0.0002
  G_optimizer_reuse: true
  G_optimizer_type: adam
  G_optimizer_wd: 0
  G_param_strict: true
  G_regularizer_clipstep: null
  G_regularizer_orthstep: null
  G_scheduler_gamma: 0.5
  G_scheduler_milestones:
  - 250000
  - 400000
  - 450000
  - 475000
  - 500000
  G_scheduler_type: MultiStepLR
  checkpoint_print: 200
  checkpoint_save: 5000
  checkpoint_test: 5000

[2025-01-25 00:54:33 swinir_sr_classical_patch48_x2] (main.py 61): INFO {"cfg": "configs/SR/X2/train_swinir_sr_classical_x2.yaml", "output": "Info/", "env": "default", "dist": false}
[2025-01-25 00:54:33 swinir_sr_classical_patch48_x2] (main_train.py 14): INFO Building model: swinir --> swinir_sr_classical_patch48_x2
[2025-01-25 00:56:22 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 00:56:22 swinir_sr_classical_patch48_x2] (main.py 60): INFO datasets:
  dataloader_batch_size: 32
  dataloader_num_workers: 8
  dataloader_shuffle: true
  dataset_type: sr
  train:
    H_size: 96
    dataroot_H: E:\Data\DIV2K\DIV2K_train_HR
    dataroot_L: E:\Data\DIV2K\DIV2K_train_LR_unknown\X2
  val:
    dataroot_H: E:\Data\Set14\original
    dataroot_L: E:\Data\Set14\LRbicx2
dist: false
gpu_ids:
- 0
- 1
model: swinir
n_channels: 3
net:
  depths:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  embed_dim: 180
  img_range: 1.0
  img_size: 48
  in_chans: 3
  init_type: default
  mlp_ratio: 2
  num_heads:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  resi_connection: 1conv
  type: swinir
  upsampler: pixelshuffle
  upscale: 2
  window_size: 8
path:
  checkpoint_path: Info\default\swinir_sr_classical_patch48_x2\checkpoints
  config_path: Info\default\swinir_sr_classical_patch48_x2\config.json
  pretrained_net: null
  root_path: Info\default\swinir_sr_classical_patch48_x2
scale: 2
seed: 0
task: swinir_sr_classical_patch48_x2
train:
  E_decay: 0.999
  E_param_strict: true
  G_lossfn_type: l1
  G_lossfn_weight: 1.0
  G_optimizer_clipgrad: null
  G_optimizer_lr: 0.0002
  G_optimizer_reuse: true
  G_optimizer_type: adam
  G_optimizer_wd: 0
  G_param_strict: true
  G_regularizer_clipstep: null
  G_regularizer_orthstep: null
  G_scheduler_gamma: 0.5
  G_scheduler_milestones:
  - 250000
  - 400000
  - 450000
  - 475000
  - 500000
  G_scheduler_type: MultiStepLR
  checkpoint_print: 200
  checkpoint_save: 5000
  checkpoint_test: 5000

[2025-01-25 00:56:22 swinir_sr_classical_patch48_x2] (main.py 61): INFO {"cfg": "configs/SR/X2/train_swinir_sr_classical_x2.yaml", "output": "Info/", "env": "default", "dist": false}
[2025-01-25 00:56:22 swinir_sr_classical_patch48_x2] (main_train.py 14): INFO Building model: swinir --> swinir_sr_classical_patch48_x2
[2025-01-25 00:59:16 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 00:59:16 swinir_sr_classical_patch48_x2] (main.py 60): INFO datasets:
  dataloader_batch_size: 32
  dataloader_num_workers: 8
  dataloader_shuffle: true
  dataset_type: sr
  train:
    H_size: 96
    dataroot_H: E:\Data\DIV2K\DIV2K_train_HR
    dataroot_L: E:\Data\DIV2K\DIV2K_train_LR_unknown\X2
  val:
    dataroot_H: E:\Data\Set14\original
    dataroot_L: E:\Data\Set14\LRbicx2
dist: false
gpu_ids:
- 0
- 1
model: swinir
n_channels: 3
net:
  depths:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  embed_dim: 180
  img_range: 1.0
  img_size: 48
  in_chans: 3
  init_type: default
  mlp_ratio: 2
  num_heads:
  - 6
  - 6
  - 6
  - 6
  - 6
  - 6
  resi_connection: 1conv
  type: swinir
  upsampler: pixelshuffle
  upscale: 2
  window_size: 8
path:
  checkpoint_path: Info\default\swinir_sr_classical_patch48_x2\checkpoints
  config_path: Info\default\swinir_sr_classical_patch48_x2\config.json
  pretrained_net: null
  root_path: Info\default\swinir_sr_classical_patch48_x2
scale: 2
seed: 0
task: swinir_sr_classical_patch48_x2
train:
  E_decay: 0.999
  E_param_strict: true
  G_lossfn_type: l1
  G_lossfn_weight: 1.0
  G_optimizer_clipgrad: null
  G_optimizer_lr: 0.0002
  G_optimizer_reuse: true
  G_optimizer_type: adam
  G_optimizer_wd: 0
  G_param_strict: true
  G_regularizer_clipstep: null
  G_regularizer_orthstep: null
  G_scheduler_gamma: 0.5
  G_scheduler_milestones:
  - 250000
  - 400000
  - 450000
  - 475000
  - 500000
  G_scheduler_type: MultiStepLR
  checkpoint_print: 200
  checkpoint_save: 5000
  checkpoint_test: 5000

[2025-01-25 00:59:16 swinir_sr_classical_patch48_x2] (main_train.py 14): INFO Building model: swinir --> swinir_sr_classical_patch48_x2
[2025-01-25 00:59:25 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 00:59:25 swinir_sr_classical_patch48_x2] (main_train.py 14): INFO Building model: swinir --> swinir_sr_classical_patch48_x2
[2025-01-25 01:01:04 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 01:01:04 swinir_sr_classical_patch48_x2] (main_train.py 14): INFO Building model: swinir --> swinir_sr_classical_patch48_x2
[2025-01-25 01:01:04 swinir_sr_classical_patch48_x2] (main_train.py 19): INFO number of params: 11752487
[2025-01-25 01:01:16 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 01:01:16 swinir_sr_classical_patch48_x2] (main_train.py 14): INFO Building model: swinir --> swinir_sr_classical_patch48_x2
[2025-01-25 01:01:16 swinir_sr_classical_patch48_x2] (main_train.py 19): INFO Number of params: 11752487
[2025-01-25 01:01:46 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 01:01:46 swinir_sr_classical_patch48_x2] (main_train.py 14): INFO Building model: swinir --> swinir_sr_classical_patch48_x2
[2025-01-25 01:01:47 swinir_sr_classical_patch48_x2] (main_train.py 19): INFO Number of params: 11752487
[2025-01-25 01:02:16 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 01:02:16 swinir_sr_classical_patch48_x2] (main_train.py 14): INFO Building model: swinir --> swinir_sr_classical_patch48_x2
[2025-01-25 01:02:16 swinir_sr_classical_patch48_x2] (main_train.py 19): INFO Number of params: 11752487
[2025-01-25 01:05:00 swinir_sr_classical_patch48_x2] (main.py 57): INFO Full config saved to Info\default\swinir_sr_classical_patch48_x2\config.json
[2025-01-25 01:05:00 swinir_sr_classical_patch48_x2] (main_train.py 14): INFO Building model: swinir --> swinir_sr_classical_patch48_x2
[2025-01-25 01:05:00 swinir_sr_classical_patch48_x2] (main_train.py 19): INFO Number of params: 11752487
